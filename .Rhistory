knitr::opts_chunk$set(echo = TRUE)
df = read.csv("UserCarData.csv")
dim(df)
suppressPacksoldStartupMesssolds(library(tidyverse))
infodensity <- nearZeroVar(df, saveMetrics= TRUE); infodensity
df <- df[,-1]
train.rows <- sample(1:nrow(df), 0.70*nrow(df))
TRAIN <- df[train.rows,]
HOLDOUT <- df[-train.rows,]
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
df <- df[,-1]
train.rows <- sample(1:nrow(df), 0.70*nrow(df))
TRAIN <- df[train.rows,]
HOLDOUT <- df[-train.rows,]
infodensity <- nearZeroVar(df, saveMetrics= TRUE); infodensity
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
knitr::opts_chunk$set(echo = TRUE)
df = read.csv("UserCarData.csv")
dim(df)
suppressPacksoldStartupMesssolds(library(tidyverse))
suppressPacksoldStartupMesssolds(library(tidyverse))
knitr::opts_chunk$set(echo = TRUE)
df = read.csv("UserCarData.csv")
dim(df)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(doParallel))
df[sapply(df,is.character)] <- lapply(df[sapply(df,is.character)],as.factor) # Changes all character types to factors for each column
df$year <- as.factor(df$year) # Changes int year to factor year
df <- df %>%
mutate(selling_price = selling_price * 0.01,
mi_driven = round(km_driven * 0.6213712)) %>%
select(-c(km_driven,State.or.Province,City,torque,)) # Converts Rubies to Dollars, Converts Km to Miles, and removes the Km column
summary(df)
glimpse(df)
df <- df[,-1]
train.rows <- sample(1:nrow(df), 0.70*nrow(df))
TRAIN <- df[train.rows,]
HOLDOUT <- df[-train.rows,]
infodensity <- nearZeroVar(df, saveMetrics= TRUE); infodensity
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
GLM <- train(sold ~., data = TRAIN, method = "glm", trControl = fitControl, preProc = c("center","scale"))
GLM$results
postResample(predict(GLM,newdata = HOLDOUT), HOLDOUT$sold)
varImp(GLM)
df <- df %>%
mutate(selling_price = selling_price * 0.01,
mi_driven = round(km_driven * 0.6213712)) %>%
select(-c(km_driven,State.or.Province,City,torque,Region)) # Converts Rubies to Dollars, Converts Km to Miles, and removes the Km column
df = read.csv("UserCarData.csv")
dim(df)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(doParallel))
df[sapply(df,is.character)] <- lapply(df[sapply(df,is.character)],as.factor) # Changes all character types to factors for each column
df$year <- as.factor(df$year) # Changes int year to factor year
df <- df %>%
mutate(selling_price = selling_price * 0.01,
mi_driven = round(km_driven * 0.6213712)) %>%
select(-c(km_driven,State.or.Province,City,torque,Region)) # Converts Rubies to Dollars, Converts Km to Miles, and removes the Km column
df <- df[,-1]
train.rows <- sample(1:nrow(df), 0.70*nrow(df))
TRAIN <- df[train.rows,]
HOLDOUT <- df[-train.rows,]
infodensity <- nearZeroVar(df, saveMetrics= TRUE); infodensity
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
GLM <- train(sold ~., data = TRAIN, method = "glm", trControl = fitControl, preProc = c("center","scale"))
GLM$results
postResample(predict(GLM,newdata = HOLDOUT), HOLDOUT$sold)
varImp(GLM)
glmnetGrid <- expand.grid(alpha = seq(0,1,.05),lambda = 10^seq(-4,-1,length=10))
GLMnet <- train(sold~.,data=TRAIN,method='glmnet', tuneGrid=glmnetGrid, trControl=fitControl, preProc = c("center", "scale"))
plot(GLMnet)
GLMnet$results[rownames(GLMnet$bestTune),]
postResample(predict(GLMnet,newdata=HOLDOUT),HOLDOUT$sold)
varImp(GLMnet)
treeGrid <- expand.grid(cp=10^seq(-5,-1,length=25))
TREE <- train(sold~.,data=TRAIN,method='rpart', tuneGrid=treeGrid,trControl=fitControl, preProc = c("center", "scale"))
plot(TREE)
TREE$results[rownames(TREE$bestTune),]
postResample(predict(TREE,newdata=HOLDOUT),HOLDOUT$sold)
forestGrid <- expand.grid(mtry=c(1,2,3,4,5,6,7,8,9,10,11,12,13))
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
FOREST <- train(sold~.,data=TRAIN,method='rf',tuneGrid=forestGrid, trControl=fitControl, preProc = c("center", "scale"))
stopCluster(cluster)
registerDoSEQ()
plot(FOREST)
FOREST$results[rownames(FOREST$bestTune),]
postResample(predict(FOREST,newdata=HOLDOUT),HOLDOUT$sold)
varImp(FOREST)
svmLinearGrid <- expand.grid(C=2^(1:10) )
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
SVM <- train(sold~.,data=TRAIN,method='svmLinear', trControl=fitControl,tuneGrid = svmLinearGrid, preProc = c("center", "scale"))
svmLinearGrid <- expand.grid(C=2^(1:2) )
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
SVM <- train(sold~.,data=TRAIN,method='svmLinear', trControl=fitControl,tuneGrid = svmLinearGrid, preProc = c("center", "scale"))
stopCluster(cluster)
registerDoSEQ()
plot(SVM)
SVM$results[rownames(SVM$bestTune),]
postResample(predict(SVM,newdata=HOLDOUT),HOLDOUT$sold)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
knnGrid <- expand.grid(k=1:20)
KNN <- train(sold~.,data=TRAIN, method='knn', trControl=fitControl,tuneGrid=knnGrid,preProc = c("center", "scale"))
stopCluster(cluster)
registerDoSEQ()
plot(KNN)
KNN$results[rownames(KNN$bestTune),]
varImp(KNN)
postResample(predict(KNN, newdata = HOLDOUT), HOLDOUT$sold)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
knnGrid <- expand.grid(k=1:50)
KNN <- train(sold~.,data=TRAIN, method='knn', trControl=fitControl,tuneGrid=knnGrid,preProc = c("center", "scale"))
stopCluster(cluster)
registerDoSEQ()
plot(KNN)
KNN$results[rownames(KNN$bestTune),]
varImp(KNN)
postResample(predict(KNN, newdata = HOLDOUT), HOLDOUT$sold)
svmPolyGrid <- expand.grid(degree=3:5, scale=10^seq(-4,-1,by=1), C=2^(2:5) )
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
SVMpoly <- train(sold~.,data=TRAIN,method='svmPoly', trControl=fitControl,tuneGrid = svmPolyGrid,preProc = c("center", "scale"))
>>>>>>> origin/main
knitr::opts_chunk$set(echo = TRUE)
xgboostGrid <- expand.grid(eta=c(0.1,0.001),nrounds=c(50,100,200,500,1000,1500),max_depth=1:5, min_child_weight=1, gamma=c(.1,.5,0,1,5,10), colsample_bytree=c(0.6,0.8,1), subsample=c(0.6,0.8,1.0))
cluster <- makeCluster(detectCores() - 1)
GLM <- train(sold ~., data = TRAIN, method = "glm", trControl = fitControl, preProc = c("center","scale"))
library(caret)
GLM <- train(sold ~., data = TRAIN, method = "glm", trControl = fitControl, preProc = c("center","scale"))
GLM$results
postResample(predict(GLM,newdata = HOLDOUT), HOLDOUT$sold)
varImp(GLM)
varImp(GLM)
knitr::opts_chunk$set(echo = TRUE)
#packages
library(dplyr)
library(ggplot2)
library(gridExtra)
library(tidyr)
transaction.data <- read.csv("transaction_data.csv")
product.data <- read.csv("product.csv")
##########################################################3
#join the datasets
names(product.data) <- tolower(names(product.data))
names(transaction.data) <- tolower(names(transaction.data))
transactions_joined <- transaction.data %>% left_join(product.data, by="product_id")
head(transactions_joined)
#made the assumption we want weekly sales revenue
comparison <- transactions_joined %>%
filter(store_id %in% c(381,292)) %>%
group_by(week_no, store_id) %>%
summarise(sales.weekly = sum(sales_value)); comparison
ggplot(comparison, aes(x=week_no, y= sales.weekly, col = as.factor(store_id))) + geom_line()
t.test(sales.weekly~store_id, data = comparison, paired = TRUE)
comparison2 = comparison %>%
pivot_wider( names_from = "store_id", values_from = "sales.weekly") %>%
mutate(difference = `292` - `381`)
#test for autocorrelation
acf(comparison2$difference, main= "")
acf(comparison2$difference, lag.max = 20, plot = FALSE)
#thus, there is significant autocorrelation and we must recalculate the CI with a new SE
#recalculating the SE
#recalculate the CI
standard.error = sd(comparison2$difference)/sqrt(length(comparison2$difference))
rho = 0.372 # maximum observed autocorrelation at lag=4
corrected.standard.error = sqrt((1+rho)/(1-rho))*standard.error
mean(comparison2$difference) + c(-1,1)*qt(.975,length(comparison2$difference)-1)*corrected.standard.error
t.test(x = comparison2$`381`, y = comparison2$`292`, paired = TRUE, conf.level = 0.95 )
comparison
View(comparison)
View(comparison2)
t.test(sales.weekly~store_id, data = comparison2, paired = TRUE)
t.test(sales.weekly~store_id, data = comparison, paired = TRUE)
t.test(sales.weekly~store_id, data = comparison, paired = TRUE)
standard.error = sd(comparison2$difference)/sqrt(length(comparison2$difference))
rho = 0.372 # maximum observed autocorrelation at lag=4
corrected.standard.error = sqrt((1+rho)/(1-rho))*standard.error
mean(comparison2$difference) + c(-1,1)*qt(.975,length(comparison2$difference)-1)*corrected.standard.error
t.test(x = comparison2$`381`, y = comparison2$`292`, paired = TRUE, conf.level = 0.95 )
#made the assumption we want weekly sales revenue
comparison <- transactions_joined %>%
filter(store_id %in% c(381,292)) %>%
group_by(week_no, store_id) %>%
summarise(sales.weekly = sum(sales_value)); comparison
ggplot(comparison, aes(x=week_no, y= sales.weekly, col = as.factor(store_id))) + geom_line()
t.test(sales.weekly~store_id, data = comparison, paired = TRUE)
comparison2 = comparison %>%
pivot_wider( names_from = "store_id", values_from = "sales.weekly") %>%
mutate(difference = `292` - `381`)
#test for autocorrelation
acf(comparison2$difference, main= "")
acf(comparison2$difference, lag.max = 20, plot = FALSE)
#thus, there is significant autocorrelation and we must recalculate the CI with a new SE
#recalculating the SE
#recalculate the CI
standard.error = sd(comparison2$difference)/sqrt(length(comparison2$difference))
rho = 0.372 # maximum observed autocorrelation at lag=4
corrected.standard.error = sqrt((1+rho)/(1-rho))*standard.error
mean(comparison2$difference) + c(-1,1)*qt(.975,length(comparison2$difference)-1)*corrected.standard.error
t.test(x = comparison2$`381`, y = comparison2$`292`, paired = TRUE, conf.level = 0.95 )
#made the assumption we want weekly sales revenue
comparison <- transactions_joined %>%
filter(store_id %in% c(381,292)) %>%
group_by(week_no, store_id) %>%
summarise(sales.weekly = sum(sales_value)); comparison
ggplot(comparison, aes(x=week_no, y= sales.weekly, col = as.factor(store_id))) + geom_line()
t.test(sales.weekly~store_id, data = comparison, paired = TRUE)
comparison2 = comparison %>%
pivot_wider( names_from = "store_id", values_from = "sales.weekly") %>%
mutate(difference = `292` - `381`)
#test for autocorrelation
acf(comparison2$difference, main= "")
acf(comparison2$difference, lag.max = 20, plot = FALSE)
#thus, there is significant autocorrelation and we must recalculate the CI with a new SE
#recalculating the SE
#recalculate the CI
standard.error = sd(comparison2$difference)/sqrt(length(comparison2$difference))
rho = 0.372 # maximum observed autocorrelation at lag=4
corrected.standard.error = sqrt((1+rho)/(1-rho))*standard.error
mean(comparison2$difference) + c(-1,1)*qt(.975,length(comparison2$difference)-1)*corrected.standard.error
# t.test(x = comparison2$`381`, y = comparison2$`292`, paired = TRUE, conf.level = 0.95 )
#test for autocorrelation
acf(comparison2$difference, main= "")
acf(comparison2$difference, lag.max = 20, plot = FALSE)
#test for autocorrelation
acf(comparison2$difference, main= "")
acf(comparison2$difference, lag.max = 20, plot = FALSE)
acf(comparison2$difference, plot = FALSE)
acf(comparison2$difference, lag.max = 20, plot = FALSE)
t.test(sales.weekly~store_id, data = comparison, paired = TRUE)
#made the assumption we want weekly sales revenue
summary(comparison)
expense <- spend %>%
group_by(household_key,household_size) %>%
summarise(expenses = sum(spend))
spend <- read.csv('journey demographics with spend.csv')
glimpse(spend)
#i
expense <- spend %>%
group_by(household_key,household_size) %>%
summarise(expenses = sum(spend))
ggplot(expense, aes(x = household_size, y = expenses)) + geom_boxplot() + theme_classic() + labs(title = 'Expenses by household size')
#ii
#hypothesis test
model.fit = aov(expenses~household_size, data = expense)
TukeyHSD(model.fit, conf.level = 0.95, ordered = TRUE)
model.fit = aov(expenses~household_size, data = expense)
TukeyHSD(model.fit, conf.level = 0.95, ordered = TRUE)
ggplot(expense, aes(x = household_size, y = expenses)) + geom_boxplot() + theme_classic() + labs(title = 'Expenses by household size')
library(multcompView)
model.fit = aov(expenses~household_size, data = expense)
TUKEY <- TukeyHSD(model.fit, conf.level = 0.95, ordered = TRUE); TUKEY
multcompLetters4(model.fit,TUKEY)
model.fit
summary(model.fit)
multcompLetters4(model.fit,TUKEY)
summary(model.fit)
GoodBellyData <- read.csv('GoodBellyData.csv')
## we want to see if the placement of the product in the store, Endcap = 1 leads to improved sales (measured in units sold per week). Thus we should group by 0,1 in the Endcap column and compare their total sales
#remove the dates that do not have both 0 and 1 in Endcap column so that you can compare each weeks difference in sales based on if Endcap was used in stores or not
GoodBellyData2 <- GoodBellyData
GoodBellyData2 <- GoodBellyData2[!(GoodBellyData2$Date %in% c('5/11/2010','5/4/2010','5/18/2010')), ]
placement <- GoodBellyData2 %>%
group_by(Date, Endcap) %>%
summarise(quantity_sold = mean(Units.Sold)) %>%
mutate(difference = quantity_sold-lead(quantity_sold,1))
ggplot(placement, aes(x = as.factor(Endcap), y = quantity_sold)) + geom_boxplot() + theme_classic() +labs(title = 'The difference in mean quantity sold per week', x = 'Endcap')
ggplot(placement, aes(x = Date, y = quantity_sold, col = as.factor(Endcap))) + geom_point() + theme_classic()
#summary stats
summary(placement$quantity_sold[placement$Endcap==0])
summary(placement$quantity_sold[placement$Endcap==1])
t.test(placement$quantity_sold[placement$Endcap==1],placement$quantity_sold[placement$Endcap==0], paired = TRUE)
top.sodas
top.sodas <- transactions_joined %>%
group_by(basket_id) %>%
summarise(baskets.with.1053690 = sum(any(product_id == 1053690)), baskets.with.844165 = sum(any(product_id == 844165)))
#summary
summary(top.sodas)
#testing the difference between the proportions of the two different products
prop.test(x=c(sum(top.sodas$baskets.with.1053690),sum(top.sodas$baskets.with.844165)),n = c(nrow(top.sodas),nrow(top.sodas)), alternative = 'greater')
top.sodas
prop.test(x=c(sum(top.sodas$baskets.with.1053690),sum(top.sodas$baskets.with.844165)),n = c(nrow(top.sodas),nrow(top.sodas)), alternative = 'greater')
top.sodas
#summary
summary(top.sodas)
GoodBellyData <- read.csv('GoodBellyData.csv')
## we want to see if the placement of the product in the store, Endcap = 1 leads to improved sales (measured in units sold per week). Thus we should group by 0,1 in the Endcap column and compare their total sales
#remove the dates that do not have both 0 and 1 in Endcap column so that you can compare each weeks difference in sales based on if Endcap was used in stores or not
GoodBellyData2 <- GoodBellyData
GoodBellyData2 <- GoodBellyData2[!(GoodBellyData2$Date %in% c('5/11/2010','5/4/2010','5/18/2010')), ]
placement <- GoodBellyData2 %>%
group_by(Date, Endcap) %>%
summarise(quantity_sold = mean(Units.Sold)) %>%
mutate(difference = quantity_sold-lead(quantity_sold,1))
ggplot(placement, aes(x = as.factor(Endcap), y = quantity_sold)) + geom_boxplot() + theme_classic() +labs(title = 'The difference in mean quantity sold per week', x = 'Endcap')
ggplot(placement, aes(x = Date, y = quantity_sold, col = as.factor(Endcap))) + geom_point() + theme_classic()
#summary stats
summary(placement$quantity_sold[placement$Endcap==0])
summary(placement$quantity_sold[placement$Endcap==1])
t.test(placement$quantity_sold[placement$Endcap==1],placement$quantity_sold[placement$Endcap==0], paired = TRUE)
GoodBellyData2
TUKEY
# summary(model.fit)
multcompLetters4(model.fit,TUKEY)
t.test(placement$quantity_sold[placement$Endcap==1],placement$quantity_sold[placement$Endcap==0], paired = TRUE)
ggplot(placement, aes(x = as.factor(Endcap), y = quantity_sold)) + geom_boxplot() + theme_classic() +labs(title = 'The difference in mean quantity sold per week', x = 'Endcap')
ggplot(placement, aes(x = Date, y = quantity_sold, col = as.factor(Endcap))) + geom_point() + theme_classic()
ggplot(placement, aes(x = Date, y = quantity_sold, col = as.factor(Endcap))) + geom_line() + theme_classic()
ggplot(placement, aes(x = Date, y = quantity_sold, col = as.factor(Endcap))) + geom_point() + theme_classic()
install.packages(c("fastDummies", "recipes"))
\
install.packages(c("fastDummies", "recipes"))
install.packages(c("fastDummies", "recipes"))
install.packages(c("fastDummies", "recipes"))
knitr::opts_chunk$set(echo = TRUE)
library(fastDummies)
df_dummies =
dummy_columns(df, select_columns = c('name','fuel','seller_type','transmission','owner','seats'))
View(df_dummies)
library(tidyverse)
df_dummies = df_dummies %>%
select(-c('name','fuel','seller_type','transmission','owner','seats'))
df_dummies = df_dummies %>%
select(-c(name,fuel,seller_type,transmission,owner,seats))
df_dummies = df_dummies %>%
select(df_dummies,-c(name,fuel,seller_type,transmission,owner,seats))
df_dummies =
select(df_dummies,-c(name,fuel,seller_type,transmission,owner,seats))
df_dummies = select(df_dummies,-c(name,fuel,seller_type,transmission,owner,seats))
df_dummies = select(df_dummies,-c('name','fuel','seller_type','transmission','owner','seats'))
'
df_dummies = subset(df_dummies, !(df_dummies %in% drop))
View(df_dummies)
df_dummies = subset(df_dummies, select = c('name','fuel','seller_type','transmission','owner','seats')))
df_dummies = subset(df_dummies, select = c(name,fuel,seller_type,transmission,owner,seats))
df_dummies =
dummy_columns(df, select_columns = c('name','fuel','seller_type','transmission','owner','seats'))
df_dummies = subset(df_dummies, select = -c(name,fuel,seller_type,transmission,owner,seats))
glimpse(df_dummies)
sample <- sample(c(TRUE, FALSE), nrow(df_dummies), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
train <- df_dummies[sample, ]
test <- df_dummies[!sample, ]
#general linear model (glm)
model <- glm(default~student+balance+income, family="binomial", data=train)
#general linear model (glm)
model <- glm(sold~ ., family="binomial", data=train)
#general linear model (glm)
model <- glm(sold~ mi_driven, family="binomial", data=train)
model
options(scipen=999)
#general linear model (glm)
model <- glm(sold~ mi_driven, family="binomial", data=train)
model
model <- glm(sold~ ., family="binomial", data=train)
model
summary(model)
#like r^2
pscl::pR2(model)["McFadden"]
install.packages('pscl')
library(pscl)
#like r^2
pR2(model)["McFadden"]
model <- glm(sold~ mi_driven, family="binomial", data=train)
#like r^2
pR2(model)["McFadden"]
#train and test data
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
train <- df_dummies[sample, ]
View(df_dummies)
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
train <- df[sample, ]
test <- df[!sample, ]
model <- glm(sold~ ., family="binomial", data=train)
summary(model)
#like r^2
pR2(model)["McFadden"]
model <- glm(sold~ ., family="binomial", data=train)
summary(model)
#like r^2
pR2(model)["McFadden"]
library (caret)
#how important is each variable
varImp(model)
predict(model, test, type="response")
test <- df[!sample, ]
predict(model, test, type="response")
library(fastDummies)
library(tidyverse)
# df_dummies =
#   dummy_columns(df, select_columns = c('name','fuel','seller_type','transmission','owner','seats'))
#
#
# df_dummies = subset(df_dummies, select = -c(name,fuel,seller_type,transmission,owner,seats))
# glimpse(df_dummies)
#train and test data
sample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
train <- df[sample, ]
test <- df[!sample, ]
options(scipen=999)
#general linear model (glm)
model <- glm(sold~ ., family="binomial", data=train)
summary(model)
#like r^2 this value needs to be above 0.4 to be significant
pR2(model)["McFadden"]
#not very high, notice the only significant indicator is selling price
#how important is each variable
varImp(model)
predict(model, test, type="response")
model <- glm(sold~ .-name, family="binomial", data=train)
summary(model)
predict(model, test, type="response")
model <- glm(sold~ .-name, family="binomial", data=train)
summary(model)
model2 <- glm(sold~ ., family="binomial", data=train)
summary(model2)
predict(model2, test, type="response")
model2 <- glm(sold~ .-name, family="binomial", data=train)
summary(model2)
predict(model2, test, type="response")
model2
predict(model2, test, type="response")
# df_dummies =
#   dummy_columns(df, select_columns = c('name','fuel','seller_type','transmission','owner','seats'))
#
#
df_logistic = subset(df_dummies, select = -c(name))
# df_dummies =
#   dummy_columns(df, select_columns = c('name','fuel','seller_type','transmission','owner','seats'))
#
#
df_logistic = subset(df, select = -c(name))
sample <- sample(c(TRUE, FALSE), nrow(df_logistic), replace=TRUE, prob=c(0.7,0.3))
train <- df_logistic[sample, ]
test <- df_logistic[!sample, ]
model2 <- glm(sold~ ., family="binomial", data=train)
summary(model2)
predict(model2, test, type="response")
predicted = predict(model2, test, type="response")
predicted = predict(model2, test, type="response")
test$sold <- ifelse(test$sold=="Yes", 1, 0)
View(test)
# df_dummies =
#   dummy_columns(df, select_columns = c('name','fuel','seller_type','transmission','owner','seats'))
#
#
df_logistic = subset(df, select = -c(name))
sample <- sample(c(TRUE, FALSE), nrow(df_logistic), replace=TRUE, prob=c(0.7,0.3))
train <- df_logistic[sample, ]
test <- df_logistic[!sample, ]
#optimal cutoff
library(InformationValue)
install.packages('InformationValue')
#optimal cutoff
library(InformationValue)
optimal <- optimalCutoff(test$sell, predicted)[1]
optimalCutoff
optimal
optimal
predicted = predict(model2, test, type="response")
test$sell <- ifelse(test$sell=="Yes", 1, 0)
model2 <- glm(sold~ ., family="binomial", data=train)
summary(model2)
predicted = predict(model2, test, type="response")
predicted
test$sell <- ifelse(test$sell=="Yes", 1, 0)
test$sell <- ifelse(test$sold=="Y", 1, 0)
optimal <- optimalCutoff(test$sell, predicted)[1]
optimal
confusionMatrix(test$sell, predicted)
confusionMatrix(test$sold, predicted)
confusionMatrix(test$sell, predicted)
predicted = predict(model2, test, type="response")
confusionMatrix(test$sell, predicted)
test$sold <- ifelse(test$sold=="Y", 1, 0)
optimal <- optimalCutoff(test$sold, predicted)[1]
optimal
confusionMatrix(test$sell, predicted)
#error rate
misClassError(test$sold, predicted, threshold=optimal)
#acuracy rate
1-misClassError(test$default, predicted, threshold=optimal)
#acuracy rate
1-(misClassError(test$default, predicted, threshold=optimal))
#acuracy rate
1-(misClassError(test$sold, predicted, threshold=optimal))
confusionMatrix(test$sell, predicted)
#error rate
misClassError(test$sold, predicted, threshold=optimal)
#acuracy rate
1-(misClassError(test$sold, predicted, threshold=optimal))
summary(model2)
nnetGrid <- expand.grid(size=1:6,decay=10^( seq(-5,5,length=10) ) )
cluster <- makeCluster(detectCores() - 1)
svmRadialGrid <- expand.grid(sigma=10^seq(-1,5,by=1), C=2^(0:12) )
cluster <- makeCluster(detectCores() - 1)
GLM <- train(sold ~., data = TRAIN, method = "glm", trControl = fitControl, preProc = c("center","scale"))
GLM$results
postResample(predict(GLM,newdata = HOLDOUT), HOLDOUT$sold)
varImp(GLM)
#acuracy rate
1-(misClassError(test$sold, predicted, threshold=optimal))
summary(GLM)
summary(model2)
